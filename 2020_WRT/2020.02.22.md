
# TIL

오늘은 Dropout과 batchnormalize를 적용해서 네트워크가 수정된것을 확인할 수 있었다.

근데 이거 적용하면 왜 성능이 좋아지지?


LSTM은 Default 활성화함수가  hyperbolic tangent (tanh)이다.
 Keras Activation
 activation: Activation function to use (see activations). Default: hyperbolic tangent (tanh). If you pass None, no activation is applied (ie. "linear" activation: a(x) = x).


 코넬대를 가고싶다.
 https://www.orie.cornell.edu/orie


마르코 로페즈.
https://mlfinlab.readthedocs.io/en/latest/index.html

https://mathinvestor.org/2019/01/marcos-lopez-de-prado-named-2019-quant-of-the-year-by-journal-of-portfolio-management/

 AQR Strategy
 https://www.aqr.com/Strategies#overview


파이썬 디버깅

https://wkdtjsgur100.github.io/pytest-description-1/



https://www.youtube.com/watch?v=hOgQDK2-lA8&feature=youtu.be&fbclid=IwAR3L7BN_y2QhH8gGOYSTfpkIQVImQR-spqxNkF3WOOCrjaWv6lT-KNH783A


Dropout도 
오버피팅을 방지하는 역할을 한다.
어떤 조건에서 동일한 역할을 하는지 살펴본다.
https://buomsoo-kim.github.io/keras/2018/04/24/Easy-deep-learning-with-Keras-5.md/

Dropout이 ra

배치노말라이제이션
https://excelsior-cjh.tistory.com/178
https://excelsior-cjh.tistory.com/178

ROC AUC 커브
https://gist.github.com/zachguo/10296432


라벨링 하랴. [0,1] 기준
다른 데이터 사용하랴. 다른 종목, 다른 시간대
노말라이즈 설정하랴. 데이터셋에 적용할 노말라이즈
모델 구조 설정하랴. CNN 모델링 구조.

왜 이렇게 나오는거지?
데이터 샘플 숫자가 달라지네?

normalize를 안하면 inbalanced 문제 발생..
normalize를 하면 inbalanced 문제 해소...


Reproducible results in Keras



Unrolling 

https://machinelearningmastery.com/rnn-unrolling/


pyfolio
https://github.com/quantopian/pyfolio


5분봉 
(795, 100, 100, 3)
(795,)



Predicting Stock Price with LSTM
https://towardsdatascience.com/predicting-stock-price-with-lstm-13af86a74944
https://github.com/DarkKnight1991/Stock-Price-Prediction/blob/master/stock_pred_main.py



일봉 
거래량.

자기 지도 학습의 예로 알려진 오토인코더는
오토인코더의 타겟은 수정하지 않은 원본 입력이다.
같은 방식으로 지난 프레임이 주어졌을 때 비디오의 다음 프레임을 예측하는 것이다. 

이전 단어가 주어졌을 때 다음 단어를 예측하는 것이 자기 지도 학습의 예이다.

오토인코더는 다깃이 있고 손실 함수를 최소화 하도록 학습되기 때문에 지도학습으로 보이지만, 입력 데이터의 ㅏ원 축소 용도로 사용될 때는 비지도 학습으로 볼 수 있다.

자

오토인코더의 구조는 우리가 볼 수 있듯이 다양할 수 있지만 일반적으로 말해서 입력을 낮은 차원으로 변환하는 인코더(encoder)와 낮은 차원에서 원래의 입력을 재구성하려는 디코더(decoder)를 포함한다.


그러므로, 이러한 모델들은 신경망으로 하여금 더 낮은 차원 공간으로 데이터를 압축하는 방법을 배우도록 하는 일종의 "병목(bottle neck)"을 중간에 제시한다.


이러한 알고리즘을 학습시킬 때, 목표는 최소한의 정보 손실로 원래의 입력을 재구성할 수 있는 것이다. 일단 모델이 훈련되면, 우리는 오토인코더의 인코더 요소(encoder component)만 사용하여 데이터를 마음대로 압축할 수 있다.

데이터를 압축.


비트코인 가격은 로그 수익률(즉, 가격 x+1과 가격 x의 차이에 대한 로그)으로 전환되고


 window size가 10인 연속 수익률이 만들어진다. 
 연속 수익률의 각 window는 [0,1] 범위까지 MinMaxScaler로 표준화된다.

 시도된 각 모델에 대해 우리는 training epoch의 각 단계에서 발생한 학습 및 테스트 데이터 세트의 loss와, 
 테스트 데이터 세트에서 추출한 무작위로 선택된 10개의 price return windows에 대한 오토인코더의 입력과 출력을 요약하여 보여줄 것이다.


lstm minibatch size unrolling


import pywt